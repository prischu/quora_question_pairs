{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# quora_question_pairs.ipynb\n",
    "# Purpose: Final Model for Kaggle's Quora Question Pairs Competition (March 2017 - May 2017)\n",
    "# Author: Priscilla Li and Rob Wang\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Load Required Python Libraries\n",
    "##########################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pylev import levenshtein\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import chardet\n",
    "import itertools\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "from scipy.stats import kurtosis\n",
    "from sklearn.externals import joblib\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import corpora, models\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Dataset\n",
    "##########################################\n",
    "#Training Dataset\n",
    "data = pd.read_csv('train.csv')\n",
    "data['question1'] = data['question1'].astype(str)\n",
    "data['question2'] = data['question2'].astype(str)\n",
    "y = data['is_duplicate']\n",
    "df_train = data\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Test Dataset\n",
    "##########################################\n",
    "#Test Dataset\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "#Replaces np.nan with ''\n",
    "df_test = df_test.replace(np.nan, '', regex=True)\n",
    "\n",
    "#Saves the cleaned test.csv\n",
    "# df_test.to_csv('cleaned_test.csv')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Initializes variables for Feature Creation\n",
    "##########################################\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "model = KeyedVectors.load(\"300features_10minwords_5context\")\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Function for Magic Features\n",
    "##########################################\n",
    "def try_apply_dict(x,dict_to_apply):\n",
    "    try:\n",
    "        return dict_to_apply[x]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def magic_features(data = df_train, test_data = df_test):\n",
    "    df1 = data[['question1']].copy()\n",
    "    df2 = data[['question2']].copy()\n",
    "    df1_test = test_data[['question1']].copy()\n",
    "    df2_test = test_data[['question2']].copy()\n",
    "\n",
    "    df2.rename(columns = {'question2':'question1'},inplace=True)\n",
    "    df2_test.rename(columns = {'question2':'question1'},inplace=True)\n",
    "\n",
    "    train_questions = df1.append(df2)\n",
    "    train_questions = train_questions.append(df1_test)\n",
    "    train_questions = train_questions.append(df2_test)\n",
    "    train_questions.drop_duplicates(subset = ['question1'],inplace=True)\n",
    "\n",
    "    train_questions.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    questions_dict = pd.Series(train_questions.index.values,index=train_questions.question1.values).to_dict()\n",
    "\n",
    "    train_cp = data.copy()\n",
    "    test_cp = test_data.copy()\n",
    "    train_cp.drop(['qid1','qid2'],axis=1,inplace=True)\n",
    "\n",
    "    test_cp['is_duplicate'] = -1\n",
    "    test_cp.rename(columns={'test_id':'id'},inplace=True)\n",
    "\n",
    "    comb = pd.concat([train_cp,test_cp])\n",
    "\n",
    "    comb['q1_hash'] = comb['question1'].map(questions_dict)\n",
    "    comb['q2_hash'] = comb['question2'].map(questions_dict)\n",
    "\n",
    "    q1_vc = comb.q1_hash.value_counts().to_dict()\n",
    "    q2_vc = comb.q2_hash.value_counts().to_dict()\n",
    "\n",
    "    #map to frequency space\n",
    "    comb['q1_freq'] = comb['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "    comb['q2_freq'] = comb['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "\n",
    "    train_comb = comb[comb['is_duplicate'] >= 0][['q1_hash','q2_hash','q1_freq','q2_freq']]\n",
    "    test_comb = comb[comb['is_duplicate'] < 0][['q1_hash','q2_hash','q1_freq','q2_freq']]\n",
    "    return np.array(train_comb), np.array(test_comb)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Function for Last Character Feature\n",
    "##########################################\n",
    "def last_character(q1,q2):\n",
    "    q1_list = q1.replace(' ', '')\n",
    "    q2_list = q2.replace(' ', '')\n",
    "    try:\n",
    "        if q1_list[-1] == q2_list[-1]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "class LastCharacter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, checks if the last character (punctuation) is equal, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        last_char = [last_character(q1,q2) for q1, q2 in zip(q1_list, q2_list)]\n",
    "\n",
    "        return np.array(last_char).reshape(len(last_char),1)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Function and Transformer for Word2Vec Features\n",
    "##########################################\n",
    "def question_to_wordlist(text, remove_stopwords = False):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "    return(words)\n",
    "\n",
    "def makeDistributionalFeatures(q1,q2):\n",
    "    data = pd.concat([q1, q2], axis=1)\n",
    "    features = []\n",
    "    \n",
    "    #For each question in the dataset:\n",
    "    # 1 - Compute similarity metric from word2vec model using every word combination between question1 and question2\n",
    "    # 2 - Create the distributional summary statistics for every combination of the disimilar words\n",
    "    for index in range(0, len(data)):\n",
    "            #Convert question1 and question2 into a list of words\n",
    "            question1 = question_to_wordlist(data.question1[index])\n",
    "            question2 = question_to_wordlist(data.question2[index])\n",
    "            \n",
    "            #Finds every word combination between question1 and question2\n",
    "            combinations = list(itertools.product(question1, question2))\n",
    "            combinations = [list(combination) for combination in combinations]\n",
    "            \n",
    "            #Tracks word2vec similarity metric for every word combination\n",
    "            values = []\n",
    "            \n",
    "            #Loops through each word combination\n",
    "            for combination in combinations:\n",
    "                #Checks if the model contains the words in its vocabulary\n",
    "                # 1 - Yes, adds it to the values list to calculate distributional stats with\n",
    "                # 2 - No, go to the next word pair\n",
    "                try:\n",
    "                    values.append(model.wv.similarity(combination[0], combination[1]))\n",
    "                except KeyError:\n",
    "                    pass\n",
    "      \n",
    "            #If there is at least one similarity metric calculate its mean and median\n",
    "            if(len(values) >= 1):\n",
    "                features.append([np.mean(values), np.median(values), np.std(values), kurtosis(values)])\n",
    "            else:\n",
    "                #Since we will not be deleting observations from the test dataset append [-1,-1,-1,-1] as stand in features\n",
    "                # 1 - The only combination contained a word our model does not contain\n",
    "                # 2 - Question1 or Question2 or both were \"\"\n",
    "                # 3 - We could not tokenize either Question1 or Question2\n",
    "                features.append([-1,-1,-1,-1])\n",
    "    return features\n",
    "\n",
    "class Word2VecStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the generates the mean/median/std/kurtosis between each string, returns array of lists\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        stats = makeDistributionalFeatures(q1_list, q2_list)\n",
    "        return np.array(stats)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self  \n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Function and Transformer for Average Shared Words\n",
    "##########################################\n",
    "def shared_words(q1,q2):\n",
    "    question1_words = []\n",
    "    question2_words = []\n",
    "\n",
    "    for word in set(q1.lower().split()):\n",
    "        if word not in stop_words:\n",
    "            question1_words.append(word)\n",
    "\n",
    "    for word in set(q2.lower().split()):\n",
    "        if word not in stop_words:\n",
    "            question2_words.append(word)\n",
    "\n",
    "    #Question contains only stop words (or is an empty string)\n",
    "    if len(question1_words) == 0 or len(question2_words) == 0:\n",
    "        return 0\n",
    "\n",
    "    question1_shared_words = [w for w in question1_words if w in question2_words]\n",
    "    question2_shared_words = [w for w in question2_words if w in question1_words]\n",
    "\n",
    "    avg_words_shared = (len(question1_shared_words) + len(question2_shared_words))/(len(question1_words) + len(question2_words))\n",
    "    return avg_words_shared\n",
    "\n",
    "class AverageSharedWords(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the average shared words between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        avg_words = [shared_words(q1,q2) for q1, q2 in zip(q1_list, q2_list)]\n",
    "\n",
    "        return np.array(avg_words).reshape(len(avg_words),1)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Function and Transformer for Word Length\n",
    "##########################################\n",
    "def word_lengths(q1,q2):\n",
    "    data = pd.concat([q1, q2], axis=1)\n",
    "\n",
    "    #Feature: Length of Question\n",
    "    data['len_q1'] = data.question1.apply(lambda x: len(x))\n",
    "    data['len_q2'] = data.question2.apply(lambda x: len(x))\n",
    "\n",
    "    #Feature: Difference in length between the Questions\n",
    "    data['len_diff'] = data.len_q1 - data.len_q2\n",
    "\n",
    "    #Feature: Character count of Question\n",
    "    data['len_char_q1'] = data.question1.apply(lambda x: len(x.replace(' ', '')))\n",
    "    data['len_char_q2'] = data.question2.apply(lambda x: len(x.replace(' ', '')))\n",
    "\n",
    "    #Feature: Word count of Question\n",
    "    data['len_word_q1'] = data.question1.apply(lambda x: len(x.split()))\n",
    "    data['len_word_q2'] = data.question2.apply(lambda x: len(x.split()))\n",
    "\n",
    "    #Feature: Common words between the Questions\n",
    "    data['len_common_words'] = data.apply(lambda x: len(set(x['question1'].lower().split()).intersection(set(x['question2'].lower().split()))), axis=1)\n",
    "    return data.ix[:,'len_q1':'len_common_words']\n",
    "\n",
    "class WordLengths(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the word lengths between each string, returns array of lists\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        word_len = word_lengths(q1_list, q2_list)\n",
    "        return np.array(word_len)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self  \n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Transformers for Levenshtein and Tfidf\n",
    "##########################################\n",
    "class LevDistanceTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        \n",
    "        lev_distance_strings = [[a,b] for a,b in zip(q1_list, q2_list)]\n",
    "        \n",
    "        lev_dist_array = np.array([(float(levenshtein(pair[0], pair[1]))/\n",
    "                                    (float(sum([x.count('') for x in pair[0]])) + float(sum([x.count('') for x in pair[1]])))) \n",
    "                                    for pair in lev_distance_strings \n",
    "                                    ])\n",
    "        \n",
    "        return lev_dist_array.reshape(len(lev_dist_array),1)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class TfIdfDiffTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the tfidf difference between each string, returns tfidf matrix\"\"\"\n",
    "\n",
    "    def __init__(self, total_words):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        total_questions = list(q1_list) + list(q2_list)\n",
    "        total_questions = [x for x in total_questions if type(x) != float]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words = 'english', vocabulary = total_words)\n",
    "        vectorizer.fit(total_questions)\n",
    "        tf_diff = vectorizer.transform(q1_list) - vectorizer.transform(q2_list)\n",
    "        return tf_diff\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Transformer for LDA\n",
    "##########################################\n",
    "class LDATransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the topics and probability for each string, returns list of lists\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        total_questions = list(q1_list) + list(q2_list)\n",
    "\n",
    "        #Tokenize each question\n",
    "        questions = [question_to_wordlist(question, remove_stopwords = True) for question in total_questions]\n",
    "\n",
    "        #Create a Gensim dictionary from the questions\n",
    "        dictionary = Dictionary(questions)\n",
    "\n",
    "        #Remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "        dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "        #Convert the dictionary to a Bag of Words corpus for reference\n",
    "        corpus = [dictionary.doc2bow(question) for question in questions]\n",
    "\n",
    "        #Train LDA model\n",
    "        topics=50\n",
    "        lda = models.LdaMulticore(corpus, id2word=dictionary, num_topics=topics, workers=150)\n",
    "\n",
    "        #Return Document Topics for Question1\n",
    "        empty = np.zeros(shape=(len(q1_list),topics*2))\n",
    "        empty[empty == 0] = -1\n",
    "\n",
    "        colNames = []\n",
    "        for i in range(0, topics):\n",
    "            colNames.append('q1_topic' + str(i))\n",
    "            colNames.append('q1_proba' + str(i))\n",
    "\n",
    "        q1_df = pd.DataFrame(empty, columns=colNames)\n",
    "\n",
    "        for x in tqdm(range(0, len(q1_list))):\n",
    "            topic_list = lda.get_document_topics(corpus[x])\n",
    "            for topic in topic_list:\n",
    "                t = topic[0]\n",
    "                p = topic[1]\n",
    "                q1_df['q1_topic'+str(t)][x] = t\n",
    "                q1_df['q1_proba'+str(t)][x] = p\n",
    "\n",
    "        #Return Document Topics for Question2\n",
    "        empty = np.zeros(shape=(len(q2_list),topics*2))\n",
    "        empty[empty == 0] = -1\n",
    "\n",
    "        colNames = []\n",
    "        for i in range(0, topics):\n",
    "            colNames.append('q2_topic' + str(i))\n",
    "            colNames.append('q2_proba' + str(i))\n",
    "\n",
    "        q2_df = pd.DataFrame(empty, columns=colNames)\n",
    "\n",
    "        for x in tqdm(range(len(q1_list), len(corpus))):\n",
    "            topic_list = lda.get_document_topics(corpus[x])\n",
    "            for topic in topic_list:\n",
    "                t = topic[0]\n",
    "                p = topic[1]\n",
    "                q2_df['q2_topic'+str(t)][x-len(q1_list)] = t\n",
    "                q2_df['q2_proba'+str(t)][x-len(q1_list)] = p\n",
    "        \n",
    "        total_df = pd.concat([q1_df, q2_df], axis=1)\n",
    "        return total_df\n",
    "    \n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use word vocabulary from training data\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "vectorizer.fit(df_train['question1'] + df_train['question2'])\n",
    "total_words = list(set(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Combining all the features using FeatureUnion\n",
    "##########################################\n",
    "#Create Magic Features\n",
    "magic_train, magic_test = magic_features()\n",
    "\n",
    "#Feature Union Features\n",
    "comb_features = FeatureUnion([('tf', TfIdfDiffTransformer(total_words)), \n",
    "                              ('lev', LevDistanceTransformer()),\n",
    "                              ('AvgWords', AverageSharedWords()),\n",
    "                              ('WordLengths', WordLengths()),\n",
    "                              ('Word2VecStats', Word2VecStats()),\n",
    "                              ('LastChar', LastCharacter())\n",
    "                             ])\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Create features using FeatureUnion and Magic Features\n",
    "##########################################\n",
    "y = df_train.ix[:,'is_duplicate']\n",
    "all_features = comb_features.transform([df_train['question1'], df_train['question2']])\n",
    "\n",
    "#Merge FeatureUnion features with Magic Features\n",
    "total_features = scipy.sparse.hstack(blocks=[all_features, magic_train])\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<404290x86722 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 10278434 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total_features.pkl']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saves Training Features\n",
    "joblib.dump(total_features, 'total_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Split the dataset into training and testing datasets\n",
    "##########################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(total_features, y, test_size=0.2)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.68391\ttest-logloss:0.683974\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[10]\ttrain-logloss:0.608342\ttest-logloss:0.609011\n",
      "[20]\ttrain-logloss:0.554719\ttest-logloss:0.555924\n",
      "[30]\ttrain-logloss:0.515015\ttest-logloss:0.516804\n",
      "[40]\ttrain-logloss:0.484505\ttest-logloss:0.486959\n",
      "[50]\ttrain-logloss:0.460981\ttest-logloss:0.464066\n",
      "[60]\ttrain-logloss:0.442954\ttest-logloss:0.446631\n",
      "[70]\ttrain-logloss:0.428278\ttest-logloss:0.432452\n",
      "[80]\ttrain-logloss:0.416599\ttest-logloss:0.421259\n",
      "[90]\ttrain-logloss:0.407014\ttest-logloss:0.412125\n",
      "[100]\ttrain-logloss:0.3989\ttest-logloss:0.404486\n",
      "[110]\ttrain-logloss:0.392254\ttest-logloss:0.398251\n",
      "[120]\ttrain-logloss:0.386747\ttest-logloss:0.393185\n",
      "[130]\ttrain-logloss:0.382262\ttest-logloss:0.389117\n",
      "[140]\ttrain-logloss:0.378257\ttest-logloss:0.385558\n",
      "[150]\ttrain-logloss:0.374701\ttest-logloss:0.382426\n",
      "[160]\ttrain-logloss:0.371639\ttest-logloss:0.379749\n",
      "[170]\ttrain-logloss:0.369079\ttest-logloss:0.377549\n",
      "[180]\ttrain-logloss:0.366875\ttest-logloss:0.375673\n",
      "[190]\ttrain-logloss:0.36475\ttest-logloss:0.373868\n",
      "[200]\ttrain-logloss:0.362912\ttest-logloss:0.372295\n",
      "[210]\ttrain-logloss:0.361236\ttest-logloss:0.370917\n",
      "[220]\ttrain-logloss:0.359782\ttest-logloss:0.369759\n",
      "[230]\ttrain-logloss:0.35852\ttest-logloss:0.368779\n",
      "[240]\ttrain-logloss:0.35737\ttest-logloss:0.36786\n",
      "[250]\ttrain-logloss:0.356332\ttest-logloss:0.367035\n",
      "[260]\ttrain-logloss:0.355329\ttest-logloss:0.366283\n",
      "[270]\ttrain-logloss:0.354295\ttest-logloss:0.365525\n",
      "[280]\ttrain-logloss:0.353474\ttest-logloss:0.364892\n",
      "[290]\ttrain-logloss:0.352611\ttest-logloss:0.364206\n",
      "[300]\ttrain-logloss:0.35187\ttest-logloss:0.363659\n",
      "[310]\ttrain-logloss:0.351186\ttest-logloss:0.363113\n",
      "[320]\ttrain-logloss:0.350526\ttest-logloss:0.362607\n",
      "[330]\ttrain-logloss:0.349889\ttest-logloss:0.362136\n",
      "[340]\ttrain-logloss:0.349349\ttest-logloss:0.361737\n",
      "[350]\ttrain-logloss:0.348716\ttest-logloss:0.361263\n",
      "[360]\ttrain-logloss:0.348223\ttest-logloss:0.360905\n",
      "[370]\ttrain-logloss:0.347723\ttest-logloss:0.360533\n",
      "[380]\ttrain-logloss:0.347198\ttest-logloss:0.360133\n",
      "[390]\ttrain-logloss:0.346658\ttest-logloss:0.35972\n",
      "[400]\ttrain-logloss:0.34616\ttest-logloss:0.359352\n",
      "[410]\ttrain-logloss:0.345612\ttest-logloss:0.358954\n",
      "[420]\ttrain-logloss:0.345184\ttest-logloss:0.358642\n",
      "[430]\ttrain-logloss:0.344772\ttest-logloss:0.358336\n",
      "[440]\ttrain-logloss:0.344224\ttest-logloss:0.357904\n",
      "[450]\ttrain-logloss:0.34377\ttest-logloss:0.357582\n",
      "[460]\ttrain-logloss:0.343393\ttest-logloss:0.357319\n",
      "[470]\ttrain-logloss:0.342966\ttest-logloss:0.357013\n",
      "[480]\ttrain-logloss:0.342531\ttest-logloss:0.356692\n",
      "[490]\ttrain-logloss:0.342145\ttest-logloss:0.356424\n",
      "[500]\ttrain-logloss:0.341753\ttest-logloss:0.356141\n",
      "[510]\ttrain-logloss:0.341343\ttest-logloss:0.355866\n",
      "[520]\ttrain-logloss:0.340971\ttest-logloss:0.355592\n",
      "[530]\ttrain-logloss:0.34058\ttest-logloss:0.355321\n",
      "[540]\ttrain-logloss:0.340207\ttest-logloss:0.355088\n",
      "[550]\ttrain-logloss:0.339852\ttest-logloss:0.35484\n",
      "[560]\ttrain-logloss:0.339481\ttest-logloss:0.354598\n",
      "[570]\ttrain-logloss:0.33913\ttest-logloss:0.354375\n",
      "[580]\ttrain-logloss:0.338684\ttest-logloss:0.354055\n",
      "[590]\ttrain-logloss:0.338363\ttest-logloss:0.353857\n",
      "[600]\ttrain-logloss:0.338064\ttest-logloss:0.353644\n",
      "[610]\ttrain-logloss:0.337734\ttest-logloss:0.353438\n",
      "[620]\ttrain-logloss:0.337439\ttest-logloss:0.353245\n",
      "[630]\ttrain-logloss:0.337043\ttest-logloss:0.352975\n",
      "[640]\ttrain-logloss:0.336784\ttest-logloss:0.352807\n",
      "[650]\ttrain-logloss:0.336502\ttest-logloss:0.352637\n",
      "[660]\ttrain-logloss:0.336183\ttest-logloss:0.352433\n",
      "[670]\ttrain-logloss:0.33566\ttest-logloss:0.352052\n",
      "[680]\ttrain-logloss:0.335392\ttest-logloss:0.351879\n",
      "[690]\ttrain-logloss:0.335105\ttest-logloss:0.351695\n",
      "[700]\ttrain-logloss:0.334821\ttest-logloss:0.351518\n",
      "[710]\ttrain-logloss:0.334572\ttest-logloss:0.351361\n",
      "[720]\ttrain-logloss:0.334225\ttest-logloss:0.351119\n",
      "[730]\ttrain-logloss:0.333889\ttest-logloss:0.350896\n",
      "[740]\ttrain-logloss:0.333643\ttest-logloss:0.350747\n",
      "[750]\ttrain-logloss:0.333376\ttest-logloss:0.350571\n",
      "[760]\ttrain-logloss:0.332954\ttest-logloss:0.350299\n",
      "[770]\ttrain-logloss:0.332658\ttest-logloss:0.350116\n",
      "[780]\ttrain-logloss:0.332363\ttest-logloss:0.349913\n",
      "[790]\ttrain-logloss:0.332069\ttest-logloss:0.349733\n",
      "[800]\ttrain-logloss:0.331837\ttest-logloss:0.349594\n",
      "[810]\ttrain-logloss:0.331562\ttest-logloss:0.349416\n",
      "[820]\ttrain-logloss:0.33131\ttest-logloss:0.349278\n",
      "[830]\ttrain-logloss:0.331051\ttest-logloss:0.349132\n",
      "[840]\ttrain-logloss:0.330814\ttest-logloss:0.348981\n",
      "[850]\ttrain-logloss:0.330559\ttest-logloss:0.348824\n",
      "[860]\ttrain-logloss:0.330314\ttest-logloss:0.348688\n",
      "[870]\ttrain-logloss:0.330006\ttest-logloss:0.348475\n",
      "[880]\ttrain-logloss:0.329754\ttest-logloss:0.34832\n",
      "[890]\ttrain-logloss:0.329492\ttest-logloss:0.348159\n",
      "[900]\ttrain-logloss:0.32922\ttest-logloss:0.347995\n",
      "[910]\ttrain-logloss:0.328981\ttest-logloss:0.347857\n",
      "[920]\ttrain-logloss:0.328733\ttest-logloss:0.347695\n",
      "[930]\ttrain-logloss:0.328471\ttest-logloss:0.347521\n",
      "[940]\ttrain-logloss:0.328256\ttest-logloss:0.347402\n",
      "[950]\ttrain-logloss:0.328054\ttest-logloss:0.34729\n",
      "[960]\ttrain-logloss:0.327813\ttest-logloss:0.347145\n",
      "[970]\ttrain-logloss:0.327512\ttest-logloss:0.346954\n",
      "[980]\ttrain-logloss:0.327279\ttest-logloss:0.346812\n",
      "[990]\ttrain-logloss:0.327068\ttest-logloss:0.346686\n",
      "[1000]\ttrain-logloss:0.326727\ttest-logloss:0.346459\n",
      "[1010]\ttrain-logloss:0.326466\ttest-logloss:0.34631\n",
      "[1020]\ttrain-logloss:0.326253\ttest-logloss:0.346188\n",
      "[1030]\ttrain-logloss:0.326021\ttest-logloss:0.346057\n",
      "[1040]\ttrain-logloss:0.325771\ttest-logloss:0.3459\n",
      "[1050]\ttrain-logloss:0.32554\ttest-logloss:0.345763\n",
      "[1060]\ttrain-logloss:0.325345\ttest-logloss:0.345654\n",
      "[1070]\ttrain-logloss:0.325111\ttest-logloss:0.345509\n",
      "[1080]\ttrain-logloss:0.32488\ttest-logloss:0.345369\n",
      "[1090]\ttrain-logloss:0.324676\ttest-logloss:0.34524\n",
      "[1100]\ttrain-logloss:0.324406\ttest-logloss:0.345086\n",
      "[1110]\ttrain-logloss:0.324192\ttest-logloss:0.34496\n",
      "[1120]\ttrain-logloss:0.323973\ttest-logloss:0.344842\n",
      "[1130]\ttrain-logloss:0.323754\ttest-logloss:0.344729\n",
      "[1140]\ttrain-logloss:0.323523\ttest-logloss:0.3446\n",
      "[1150]\ttrain-logloss:0.323312\ttest-logloss:0.344477\n",
      "[1160]\ttrain-logloss:0.32311\ttest-logloss:0.344353\n",
      "[1170]\ttrain-logloss:0.322912\ttest-logloss:0.344235\n",
      "[1180]\ttrain-logloss:0.322709\ttest-logloss:0.344092\n",
      "[1190]\ttrain-logloss:0.322462\ttest-logloss:0.343956\n",
      "[1200]\ttrain-logloss:0.32227\ttest-logloss:0.343848\n",
      "[1210]\ttrain-logloss:0.322032\ttest-logloss:0.343693\n",
      "[1220]\ttrain-logloss:0.321854\ttest-logloss:0.343597\n",
      "[1230]\ttrain-logloss:0.321662\ttest-logloss:0.34348\n",
      "[1240]\ttrain-logloss:0.321426\ttest-logloss:0.343317\n",
      "[1250]\ttrain-logloss:0.321235\ttest-logloss:0.343208\n",
      "[1260]\ttrain-logloss:0.321038\ttest-logloss:0.343098\n",
      "[1270]\ttrain-logloss:0.320818\ttest-logloss:0.342985\n",
      "[1280]\ttrain-logloss:0.320627\ttest-logloss:0.342885\n",
      "[1290]\ttrain-logloss:0.320422\ttest-logloss:0.342757\n",
      "[1300]\ttrain-logloss:0.320211\ttest-logloss:0.342643\n",
      "[1310]\ttrain-logloss:0.320034\ttest-logloss:0.342539\n",
      "[1320]\ttrain-logloss:0.319889\ttest-logloss:0.342458\n",
      "[1330]\ttrain-logloss:0.319674\ttest-logloss:0.342326\n",
      "[1340]\ttrain-logloss:0.319517\ttest-logloss:0.342249\n",
      "[1350]\ttrain-logloss:0.319333\ttest-logloss:0.342139\n",
      "[1360]\ttrain-logloss:0.319134\ttest-logloss:0.342019\n",
      "[1370]\ttrain-logloss:0.318877\ttest-logloss:0.341871\n",
      "[1380]\ttrain-logloss:0.318634\ttest-logloss:0.341727\n",
      "[1390]\ttrain-logloss:0.31846\ttest-logloss:0.341626\n",
      "[1400]\ttrain-logloss:0.318298\ttest-logloss:0.341541\n",
      "[1410]\ttrain-logloss:0.318118\ttest-logloss:0.341451\n",
      "[1420]\ttrain-logloss:0.317845\ttest-logloss:0.341302\n",
      "[1430]\ttrain-logloss:0.317682\ttest-logloss:0.341202\n",
      "[1440]\ttrain-logloss:0.317515\ttest-logloss:0.341115\n",
      "[1450]\ttrain-logloss:0.317339\ttest-logloss:0.341019\n",
      "[1460]\ttrain-logloss:0.31707\ttest-logloss:0.340852\n",
      "[1470]\ttrain-logloss:0.316911\ttest-logloss:0.340753\n",
      "[1480]\ttrain-logloss:0.316758\ttest-logloss:0.34067\n",
      "[1490]\ttrain-logloss:0.316597\ttest-logloss:0.340586\n",
      "[1500]\ttrain-logloss:0.316455\ttest-logloss:0.340518\n",
      "[1510]\ttrain-logloss:0.31626\ttest-logloss:0.340415\n",
      "[1520]\ttrain-logloss:0.316113\ttest-logloss:0.340345\n",
      "[1530]\ttrain-logloss:0.31598\ttest-logloss:0.340268\n",
      "[1540]\ttrain-logloss:0.315796\ttest-logloss:0.340159\n",
      "[1550]\ttrain-logloss:0.315633\ttest-logloss:0.340074\n",
      "[1560]\ttrain-logloss:0.31544\ttest-logloss:0.339971\n",
      "[1570]\ttrain-logloss:0.315276\ttest-logloss:0.339887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1580]\ttrain-logloss:0.315115\ttest-logloss:0.3398\n",
      "[1590]\ttrain-logloss:0.314953\ttest-logloss:0.339714\n",
      "[1600]\ttrain-logloss:0.31478\ttest-logloss:0.339627\n",
      "[1610]\ttrain-logloss:0.314656\ttest-logloss:0.339554\n",
      "[1620]\ttrain-logloss:0.314465\ttest-logloss:0.339461\n",
      "[1630]\ttrain-logloss:0.3143\ttest-logloss:0.339372\n",
      "[1640]\ttrain-logloss:0.31414\ttest-logloss:0.339284\n",
      "[1650]\ttrain-logloss:0.313984\ttest-logloss:0.339201\n",
      "[1660]\ttrain-logloss:0.313827\ttest-logloss:0.339124\n",
      "[1670]\ttrain-logloss:0.313677\ttest-logloss:0.339052\n",
      "[1680]\ttrain-logloss:0.313541\ttest-logloss:0.338982\n",
      "[1690]\ttrain-logloss:0.313404\ttest-logloss:0.338923\n",
      "[1700]\ttrain-logloss:0.313231\ttest-logloss:0.338833\n",
      "[1710]\ttrain-logloss:0.313091\ttest-logloss:0.338761\n",
      "[1720]\ttrain-logloss:0.312922\ttest-logloss:0.338681\n",
      "[1730]\ttrain-logloss:0.312768\ttest-logloss:0.338613\n",
      "[1740]\ttrain-logloss:0.312636\ttest-logloss:0.338554\n",
      "[1750]\ttrain-logloss:0.312489\ttest-logloss:0.338476\n",
      "[1760]\ttrain-logloss:0.312334\ttest-logloss:0.338413\n",
      "[1770]\ttrain-logloss:0.31219\ttest-logloss:0.338342\n",
      "[1780]\ttrain-logloss:0.312044\ttest-logloss:0.338256\n",
      "[1790]\ttrain-logloss:0.311904\ttest-logloss:0.338194\n",
      "[1800]\ttrain-logloss:0.311728\ttest-logloss:0.338091\n",
      "[1810]\ttrain-logloss:0.311568\ttest-logloss:0.338021\n",
      "[1820]\ttrain-logloss:0.311402\ttest-logloss:0.337931\n",
      "[1830]\ttrain-logloss:0.311249\ttest-logloss:0.337852\n",
      "[1840]\ttrain-logloss:0.311115\ttest-logloss:0.337794\n",
      "[1850]\ttrain-logloss:0.311009\ttest-logloss:0.337738\n",
      "[1860]\ttrain-logloss:0.310854\ttest-logloss:0.337652\n",
      "[1870]\ttrain-logloss:0.310723\ttest-logloss:0.337589\n",
      "[1880]\ttrain-logloss:0.310576\ttest-logloss:0.337522\n",
      "[1890]\ttrain-logloss:0.310425\ttest-logloss:0.337446\n",
      "[1900]\ttrain-logloss:0.310283\ttest-logloss:0.337382\n",
      "[1910]\ttrain-logloss:0.310144\ttest-logloss:0.33732\n",
      "[1920]\ttrain-logloss:0.310013\ttest-logloss:0.337257\n",
      "[1930]\ttrain-logloss:0.30989\ttest-logloss:0.337204\n",
      "[1940]\ttrain-logloss:0.30977\ttest-logloss:0.337135\n",
      "[1950]\ttrain-logloss:0.309638\ttest-logloss:0.337079\n",
      "[1960]\ttrain-logloss:0.309497\ttest-logloss:0.337004\n",
      "[1970]\ttrain-logloss:0.309337\ttest-logloss:0.33693\n",
      "[1980]\ttrain-logloss:0.309216\ttest-logloss:0.33687\n",
      "[1990]\ttrain-logloss:0.309084\ttest-logloss:0.336815\n",
      "[2000]\ttrain-logloss:0.308948\ttest-logloss:0.336757\n",
      "[2010]\ttrain-logloss:0.308815\ttest-logloss:0.336697\n",
      "[2020]\ttrain-logloss:0.308647\ttest-logloss:0.336598\n",
      "[2030]\ttrain-logloss:0.308482\ttest-logloss:0.336495\n",
      "[2040]\ttrain-logloss:0.308363\ttest-logloss:0.33644\n",
      "[2050]\ttrain-logloss:0.308238\ttest-logloss:0.336373\n",
      "[2060]\ttrain-logloss:0.308111\ttest-logloss:0.336321\n",
      "[2070]\ttrain-logloss:0.307957\ttest-logloss:0.336244\n",
      "[2080]\ttrain-logloss:0.307833\ttest-logloss:0.336179\n",
      "[2090]\ttrain-logloss:0.307684\ttest-logloss:0.336107\n",
      "[2100]\ttrain-logloss:0.30755\ttest-logloss:0.336045\n",
      "[2110]\ttrain-logloss:0.307419\ttest-logloss:0.335973\n",
      "[2120]\ttrain-logloss:0.307293\ttest-logloss:0.335916\n",
      "[2130]\ttrain-logloss:0.307192\ttest-logloss:0.335864\n",
      "[2140]\ttrain-logloss:0.30707\ttest-logloss:0.335809\n",
      "[2150]\ttrain-logloss:0.30696\ttest-logloss:0.335758\n",
      "[2160]\ttrain-logloss:0.306836\ttest-logloss:0.335713\n",
      "[2170]\ttrain-logloss:0.306693\ttest-logloss:0.335639\n",
      "[2180]\ttrain-logloss:0.306567\ttest-logloss:0.335577\n",
      "[2190]\ttrain-logloss:0.306449\ttest-logloss:0.335516\n",
      "[2200]\ttrain-logloss:0.306306\ttest-logloss:0.335439\n",
      "[2210]\ttrain-logloss:0.30617\ttest-logloss:0.335372\n",
      "[2220]\ttrain-logloss:0.306068\ttest-logloss:0.335331\n",
      "[2230]\ttrain-logloss:0.305925\ttest-logloss:0.335247\n",
      "[2240]\ttrain-logloss:0.305746\ttest-logloss:0.335152\n",
      "[2250]\ttrain-logloss:0.305629\ttest-logloss:0.335111\n",
      "[2260]\ttrain-logloss:0.305507\ttest-logloss:0.335053\n",
      "[2270]\ttrain-logloss:0.305407\ttest-logloss:0.335008\n",
      "[2280]\ttrain-logloss:0.305285\ttest-logloss:0.334951\n",
      "[2290]\ttrain-logloss:0.305177\ttest-logloss:0.334898\n",
      "[2300]\ttrain-logloss:0.305078\ttest-logloss:0.334841\n",
      "[2310]\ttrain-logloss:0.304906\ttest-logloss:0.334751\n",
      "[2320]\ttrain-logloss:0.304781\ttest-logloss:0.334691\n",
      "[2330]\ttrain-logloss:0.304659\ttest-logloss:0.334642\n",
      "[2340]\ttrain-logloss:0.304535\ttest-logloss:0.334587\n",
      "[2350]\ttrain-logloss:0.304414\ttest-logloss:0.334541\n",
      "[2360]\ttrain-logloss:0.304299\ttest-logloss:0.334497\n",
      "[2370]\ttrain-logloss:0.304174\ttest-logloss:0.334434\n",
      "[2380]\ttrain-logloss:0.303997\ttest-logloss:0.334337\n",
      "[2390]\ttrain-logloss:0.303886\ttest-logloss:0.33429\n",
      "[2400]\ttrain-logloss:0.303735\ttest-logloss:0.334206\n",
      "[2410]\ttrain-logloss:0.303622\ttest-logloss:0.334158\n",
      "[2420]\ttrain-logloss:0.303514\ttest-logloss:0.334118\n",
      "[2430]\ttrain-logloss:0.30341\ttest-logloss:0.334066\n",
      "[2440]\ttrain-logloss:0.303307\ttest-logloss:0.334018\n",
      "[2450]\ttrain-logloss:0.303196\ttest-logloss:0.333966\n",
      "[2460]\ttrain-logloss:0.303056\ttest-logloss:0.333901\n",
      "[2470]\ttrain-logloss:0.302924\ttest-logloss:0.333845\n",
      "[2480]\ttrain-logloss:0.302746\ttest-logloss:0.333739\n",
      "[2490]\ttrain-logloss:0.302649\ttest-logloss:0.333694\n",
      "[2500]\ttrain-logloss:0.302542\ttest-logloss:0.333642\n",
      "[2510]\ttrain-logloss:0.302441\ttest-logloss:0.333603\n",
      "[2520]\ttrain-logloss:0.30232\ttest-logloss:0.333546\n",
      "[2530]\ttrain-logloss:0.302217\ttest-logloss:0.333499\n",
      "[2540]\ttrain-logloss:0.302097\ttest-logloss:0.33344\n",
      "[2550]\ttrain-logloss:0.302003\ttest-logloss:0.3334\n",
      "[2560]\ttrain-logloss:0.301844\ttest-logloss:0.333328\n",
      "[2570]\ttrain-logloss:0.301717\ttest-logloss:0.333272\n",
      "[2580]\ttrain-logloss:0.301612\ttest-logloss:0.333225\n",
      "[2590]\ttrain-logloss:0.301501\ttest-logloss:0.333183\n",
      "[2600]\ttrain-logloss:0.301368\ttest-logloss:0.333126\n",
      "[2610]\ttrain-logloss:0.301259\ttest-logloss:0.333078\n",
      "[2620]\ttrain-logloss:0.301111\ttest-logloss:0.332997\n",
      "[2630]\ttrain-logloss:0.300974\ttest-logloss:0.332934\n",
      "[2640]\ttrain-logloss:0.300847\ttest-logloss:0.332883\n",
      "[2650]\ttrain-logloss:0.300714\ttest-logloss:0.332829\n",
      "[2660]\ttrain-logloss:0.300602\ttest-logloss:0.332775\n",
      "[2670]\ttrain-logloss:0.300495\ttest-logloss:0.332721\n",
      "[2680]\ttrain-logloss:0.300399\ttest-logloss:0.332685\n",
      "[2690]\ttrain-logloss:0.300291\ttest-logloss:0.332637\n",
      "[2700]\ttrain-logloss:0.300175\ttest-logloss:0.3326\n",
      "[2710]\ttrain-logloss:0.30003\ttest-logloss:0.332519\n",
      "[2720]\ttrain-logloss:0.299927\ttest-logloss:0.332472\n",
      "[2730]\ttrain-logloss:0.299828\ttest-logloss:0.332431\n",
      "[2740]\ttrain-logloss:0.299724\ttest-logloss:0.332386\n",
      "[2750]\ttrain-logloss:0.299573\ttest-logloss:0.332308\n",
      "[2760]\ttrain-logloss:0.299469\ttest-logloss:0.332265\n",
      "[2770]\ttrain-logloss:0.299343\ttest-logloss:0.332209\n",
      "[2780]\ttrain-logloss:0.299234\ttest-logloss:0.33217\n",
      "[2790]\ttrain-logloss:0.299134\ttest-logloss:0.332119\n",
      "[2800]\ttrain-logloss:0.29903\ttest-logloss:0.33208\n",
      "[2810]\ttrain-logloss:0.298902\ttest-logloss:0.332\n",
      "[2820]\ttrain-logloss:0.298797\ttest-logloss:0.331958\n",
      "[2830]\ttrain-logloss:0.298684\ttest-logloss:0.331892\n",
      "[2840]\ttrain-logloss:0.298579\ttest-logloss:0.331846\n",
      "[2850]\ttrain-logloss:0.298464\ttest-logloss:0.331804\n",
      "[2860]\ttrain-logloss:0.298365\ttest-logloss:0.331757\n",
      "[2870]\ttrain-logloss:0.29827\ttest-logloss:0.331719\n",
      "[2880]\ttrain-logloss:0.298158\ttest-logloss:0.33168\n",
      "[2890]\ttrain-logloss:0.298061\ttest-logloss:0.33164\n",
      "[2900]\ttrain-logloss:0.297955\ttest-logloss:0.331593\n",
      "[2910]\ttrain-logloss:0.297848\ttest-logloss:0.331541\n",
      "[2920]\ttrain-logloss:0.297752\ttest-logloss:0.331499\n",
      "[2930]\ttrain-logloss:0.297648\ttest-logloss:0.331451\n",
      "[2940]\ttrain-logloss:0.297475\ttest-logloss:0.331366\n",
      "[2950]\ttrain-logloss:0.297373\ttest-logloss:0.331325\n",
      "[2960]\ttrain-logloss:0.297281\ttest-logloss:0.331293\n",
      "[2970]\ttrain-logloss:0.297193\ttest-logloss:0.331261\n",
      "[2980]\ttrain-logloss:0.297049\ttest-logloss:0.3312\n",
      "[2990]\ttrain-logloss:0.296957\ttest-logloss:0.331155\n",
      "[3000]\ttrain-logloss:0.296863\ttest-logloss:0.33111\n",
      "[3010]\ttrain-logloss:0.296769\ttest-logloss:0.331064\n",
      "[3020]\ttrain-logloss:0.296686\ttest-logloss:0.331025\n",
      "[3030]\ttrain-logloss:0.296589\ttest-logloss:0.330988\n",
      "[3040]\ttrain-logloss:0.296447\ttest-logloss:0.330919\n",
      "[3050]\ttrain-logloss:0.296324\ttest-logloss:0.33086\n",
      "[3060]\ttrain-logloss:0.296222\ttest-logloss:0.330832\n",
      "[3070]\ttrain-logloss:0.296118\ttest-logloss:0.330782\n",
      "[3080]\ttrain-logloss:0.296019\ttest-logloss:0.330745\n",
      "[3090]\ttrain-logloss:0.295918\ttest-logloss:0.330711\n",
      "[3100]\ttrain-logloss:0.295792\ttest-logloss:0.330647\n",
      "[3110]\ttrain-logloss:0.295673\ttest-logloss:0.330591\n",
      "[3120]\ttrain-logloss:0.295578\ttest-logloss:0.330553\n",
      "[3130]\ttrain-logloss:0.295485\ttest-logloss:0.330516\n",
      "[3140]\ttrain-logloss:0.295377\ttest-logloss:0.330465\n",
      "[3150]\ttrain-logloss:0.295278\ttest-logloss:0.33042\n",
      "[3160]\ttrain-logloss:0.295185\ttest-logloss:0.330378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3170]\ttrain-logloss:0.295093\ttest-logloss:0.330333\n",
      "[3180]\ttrain-logloss:0.294999\ttest-logloss:0.330291\n",
      "[3190]\ttrain-logloss:0.294899\ttest-logloss:0.330251\n",
      "[3200]\ttrain-logloss:0.294818\ttest-logloss:0.330216\n",
      "[3210]\ttrain-logloss:0.294738\ttest-logloss:0.330183\n",
      "[3220]\ttrain-logloss:0.294663\ttest-logloss:0.330158\n",
      "[3230]\ttrain-logloss:0.294504\ttest-logloss:0.330068\n",
      "[3240]\ttrain-logloss:0.294408\ttest-logloss:0.330034\n",
      "[3250]\ttrain-logloss:0.294304\ttest-logloss:0.32999\n",
      "[3260]\ttrain-logloss:0.294216\ttest-logloss:0.329959\n",
      "[3270]\ttrain-logloss:0.294075\ttest-logloss:0.329875\n",
      "[3280]\ttrain-logloss:0.293967\ttest-logloss:0.32983\n",
      "[3290]\ttrain-logloss:0.293842\ttest-logloss:0.329763\n",
      "[3300]\ttrain-logloss:0.293751\ttest-logloss:0.329725\n",
      "[3310]\ttrain-logloss:0.293646\ttest-logloss:0.329688\n",
      "[3320]\ttrain-logloss:0.293558\ttest-logloss:0.329655\n",
      "[3330]\ttrain-logloss:0.293465\ttest-logloss:0.329613\n",
      "[3340]\ttrain-logloss:0.293372\ttest-logloss:0.329574\n",
      "[3350]\ttrain-logloss:0.293265\ttest-logloss:0.32953\n",
      "[3360]\ttrain-logloss:0.29317\ttest-logloss:0.329495\n",
      "[3370]\ttrain-logloss:0.293087\ttest-logloss:0.329457\n",
      "[3380]\ttrain-logloss:0.29301\ttest-logloss:0.329425\n",
      "[3390]\ttrain-logloss:0.292938\ttest-logloss:0.329394\n",
      "[3400]\ttrain-logloss:0.292824\ttest-logloss:0.329342\n",
      "[3410]\ttrain-logloss:0.29274\ttest-logloss:0.329316\n",
      "[3420]\ttrain-logloss:0.292657\ttest-logloss:0.329284\n",
      "[3430]\ttrain-logloss:0.29257\ttest-logloss:0.329246\n",
      "[3440]\ttrain-logloss:0.292448\ttest-logloss:0.329191\n",
      "[3450]\ttrain-logloss:0.292327\ttest-logloss:0.329144\n",
      "[3460]\ttrain-logloss:0.292249\ttest-logloss:0.329117\n",
      "[3470]\ttrain-logloss:0.292173\ttest-logloss:0.329091\n",
      "[3480]\ttrain-logloss:0.292074\ttest-logloss:0.329041\n",
      "[3490]\ttrain-logloss:0.291978\ttest-logloss:0.328996\n",
      "[3500]\ttrain-logloss:0.291897\ttest-logloss:0.328967\n",
      "[3510]\ttrain-logloss:0.29177\ttest-logloss:0.328897\n",
      "[3520]\ttrain-logloss:0.291687\ttest-logloss:0.328861\n",
      "[3530]\ttrain-logloss:0.291596\ttest-logloss:0.328835\n",
      "[3540]\ttrain-logloss:0.291481\ttest-logloss:0.328787\n",
      "[3550]\ttrain-logloss:0.291398\ttest-logloss:0.328761\n",
      "[3560]\ttrain-logloss:0.291315\ttest-logloss:0.328723\n",
      "[3570]\ttrain-logloss:0.291227\ttest-logloss:0.32869\n",
      "[3580]\ttrain-logloss:0.291135\ttest-logloss:0.328652\n",
      "[3590]\ttrain-logloss:0.291049\ttest-logloss:0.328615\n",
      "[3600]\ttrain-logloss:0.290974\ttest-logloss:0.328591\n",
      "[3610]\ttrain-logloss:0.290881\ttest-logloss:0.328546\n",
      "[3620]\ttrain-logloss:0.290768\ttest-logloss:0.328497\n",
      "[3630]\ttrain-logloss:0.290678\ttest-logloss:0.328463\n",
      "[3640]\ttrain-logloss:0.290602\ttest-logloss:0.328434\n",
      "[3650]\ttrain-logloss:0.290507\ttest-logloss:0.328397\n",
      "[3660]\ttrain-logloss:0.290371\ttest-logloss:0.328326\n",
      "[3670]\ttrain-logloss:0.290272\ttest-logloss:0.328295\n",
      "[3680]\ttrain-logloss:0.290173\ttest-logloss:0.328264\n",
      "[3690]\ttrain-logloss:0.290093\ttest-logloss:0.328238\n",
      "[3700]\ttrain-logloss:0.289985\ttest-logloss:0.328179\n",
      "[3710]\ttrain-logloss:0.289913\ttest-logloss:0.328158\n",
      "[3720]\ttrain-logloss:0.289835\ttest-logloss:0.328126\n",
      "[3730]\ttrain-logloss:0.289767\ttest-logloss:0.328098\n",
      "[3740]\ttrain-logloss:0.289672\ttest-logloss:0.328058\n",
      "[3750]\ttrain-logloss:0.289567\ttest-logloss:0.328009\n",
      "[3760]\ttrain-logloss:0.289483\ttest-logloss:0.327975\n",
      "[3770]\ttrain-logloss:0.289347\ttest-logloss:0.327901\n",
      "[3780]\ttrain-logloss:0.289267\ttest-logloss:0.327877\n",
      "[3790]\ttrain-logloss:0.289162\ttest-logloss:0.327818\n",
      "[3800]\ttrain-logloss:0.289086\ttest-logloss:0.327791\n",
      "[3810]\ttrain-logloss:0.289006\ttest-logloss:0.327756\n",
      "[3820]\ttrain-logloss:0.288902\ttest-logloss:0.327708\n",
      "[3830]\ttrain-logloss:0.288813\ttest-logloss:0.327675\n",
      "[3840]\ttrain-logloss:0.288722\ttest-logloss:0.327637\n",
      "[3850]\ttrain-logloss:0.288639\ttest-logloss:0.327612\n",
      "[3860]\ttrain-logloss:0.288515\ttest-logloss:0.327567\n",
      "[3870]\ttrain-logloss:0.28842\ttest-logloss:0.32753\n",
      "[3880]\ttrain-logloss:0.288339\ttest-logloss:0.327499\n",
      "[3890]\ttrain-logloss:0.288258\ttest-logloss:0.327467\n",
      "[3900]\ttrain-logloss:0.288181\ttest-logloss:0.327436\n",
      "[3910]\ttrain-logloss:0.288097\ttest-logloss:0.327399\n",
      "[3920]\ttrain-logloss:0.288008\ttest-logloss:0.327361\n",
      "[3930]\ttrain-logloss:0.287932\ttest-logloss:0.327338\n",
      "[3940]\ttrain-logloss:0.287833\ttest-logloss:0.327298\n",
      "[3950]\ttrain-logloss:0.287751\ttest-logloss:0.32727\n",
      "[3960]\ttrain-logloss:0.287677\ttest-logloss:0.327242\n",
      "[3970]\ttrain-logloss:0.287601\ttest-logloss:0.327224\n",
      "[3980]\ttrain-logloss:0.287528\ttest-logloss:0.327198\n",
      "[3990]\ttrain-logloss:0.287454\ttest-logloss:0.327169\n",
      "[4000]\ttrain-logloss:0.287377\ttest-logloss:0.327144\n",
      "[4010]\ttrain-logloss:0.28728\ttest-logloss:0.327111\n",
      "[4020]\ttrain-logloss:0.287206\ttest-logloss:0.327083\n",
      "[4030]\ttrain-logloss:0.287142\ttest-logloss:0.32706\n",
      "[4040]\ttrain-logloss:0.287057\ttest-logloss:0.327024\n",
      "[4050]\ttrain-logloss:0.28695\ttest-logloss:0.326973\n",
      "[4060]\ttrain-logloss:0.286868\ttest-logloss:0.326952\n",
      "[4070]\ttrain-logloss:0.286789\ttest-logloss:0.326925\n",
      "[4080]\ttrain-logloss:0.286721\ttest-logloss:0.326897\n",
      "[4090]\ttrain-logloss:0.286639\ttest-logloss:0.326867\n",
      "[4100]\ttrain-logloss:0.286566\ttest-logloss:0.326843\n",
      "[4110]\ttrain-logloss:0.286495\ttest-logloss:0.326814\n",
      "[4120]\ttrain-logloss:0.286414\ttest-logloss:0.326779\n",
      "[4130]\ttrain-logloss:0.286335\ttest-logloss:0.326745\n",
      "[4140]\ttrain-logloss:0.286255\ttest-logloss:0.326713\n",
      "[4150]\ttrain-logloss:0.286183\ttest-logloss:0.326685\n",
      "[4160]\ttrain-logloss:0.286106\ttest-logloss:0.32666\n",
      "[4170]\ttrain-logloss:0.286024\ttest-logloss:0.326637\n",
      "[4180]\ttrain-logloss:0.285958\ttest-logloss:0.326609\n",
      "[4190]\ttrain-logloss:0.285876\ttest-logloss:0.32658\n",
      "[4200]\ttrain-logloss:0.285787\ttest-logloss:0.326544\n",
      "[4210]\ttrain-logloss:0.285676\ttest-logloss:0.326499\n",
      "[4220]\ttrain-logloss:0.285609\ttest-logloss:0.326478\n",
      "[4230]\ttrain-logloss:0.285536\ttest-logloss:0.326449\n",
      "[4240]\ttrain-logloss:0.285453\ttest-logloss:0.326415\n",
      "[4250]\ttrain-logloss:0.285358\ttest-logloss:0.326379\n",
      "[4260]\ttrain-logloss:0.285289\ttest-logloss:0.326351\n",
      "[4270]\ttrain-logloss:0.285207\ttest-logloss:0.326307\n",
      "[4280]\ttrain-logloss:0.285118\ttest-logloss:0.326272\n",
      "[4290]\ttrain-logloss:0.285045\ttest-logloss:0.326245\n",
      "[4300]\ttrain-logloss:0.284958\ttest-logloss:0.326208\n",
      "[4310]\ttrain-logloss:0.28487\ttest-logloss:0.326177\n",
      "[4320]\ttrain-logloss:0.284786\ttest-logloss:0.32615\n",
      "[4330]\ttrain-logloss:0.284706\ttest-logloss:0.326129\n",
      "[4340]\ttrain-logloss:0.284623\ttest-logloss:0.326101\n",
      "[4350]\ttrain-logloss:0.284532\ttest-logloss:0.326067\n",
      "[4360]\ttrain-logloss:0.284459\ttest-logloss:0.326042\n",
      "[4370]\ttrain-logloss:0.284381\ttest-logloss:0.32601\n",
      "[4380]\ttrain-logloss:0.284307\ttest-logloss:0.325975\n",
      "[4390]\ttrain-logloss:0.284226\ttest-logloss:0.325938\n",
      "[4400]\ttrain-logloss:0.284135\ttest-logloss:0.325904\n",
      "[4410]\ttrain-logloss:0.284051\ttest-logloss:0.325871\n",
      "[4420]\ttrain-logloss:0.283985\ttest-logloss:0.325853\n",
      "[4430]\ttrain-logloss:0.283907\ttest-logloss:0.325833\n",
      "[4440]\ttrain-logloss:0.283807\ttest-logloss:0.325792\n",
      "[4450]\ttrain-logloss:0.283734\ttest-logloss:0.325769\n",
      "[4460]\ttrain-logloss:0.283651\ttest-logloss:0.325734\n",
      "[4470]\ttrain-logloss:0.283564\ttest-logloss:0.325702\n",
      "[4480]\ttrain-logloss:0.283491\ttest-logloss:0.325681\n",
      "[4490]\ttrain-logloss:0.283409\ttest-logloss:0.325653\n",
      "[4500]\ttrain-logloss:0.283286\ttest-logloss:0.325602\n",
      "[4510]\ttrain-logloss:0.283215\ttest-logloss:0.325585\n",
      "[4520]\ttrain-logloss:0.28311\ttest-logloss:0.32553\n",
      "[4530]\ttrain-logloss:0.28304\ttest-logloss:0.325515\n",
      "[4540]\ttrain-logloss:0.282961\ttest-logloss:0.325483\n",
      "[4550]\ttrain-logloss:0.282889\ttest-logloss:0.32546\n",
      "[4560]\ttrain-logloss:0.282819\ttest-logloss:0.325435\n",
      "[4570]\ttrain-logloss:0.282753\ttest-logloss:0.325408\n",
      "[4580]\ttrain-logloss:0.282676\ttest-logloss:0.325374\n",
      "[4590]\ttrain-logloss:0.282602\ttest-logloss:0.325346\n",
      "[4600]\ttrain-logloss:0.282529\ttest-logloss:0.325323\n",
      "[4610]\ttrain-logloss:0.282456\ttest-logloss:0.325299\n",
      "[4620]\ttrain-logloss:0.282363\ttest-logloss:0.325263\n",
      "[4630]\ttrain-logloss:0.282287\ttest-logloss:0.325232\n",
      "[4640]\ttrain-logloss:0.282212\ttest-logloss:0.325213\n",
      "[4650]\ttrain-logloss:0.282143\ttest-logloss:0.325188\n",
      "[4660]\ttrain-logloss:0.282041\ttest-logloss:0.325139\n",
      "[4670]\ttrain-logloss:0.28198\ttest-logloss:0.325119\n",
      "[4680]\ttrain-logloss:0.281872\ttest-logloss:0.325072\n",
      "[4690]\ttrain-logloss:0.281791\ttest-logloss:0.325041\n",
      "[4700]\ttrain-logloss:0.281732\ttest-logloss:0.325016\n",
      "[4710]\ttrain-logloss:0.281651\ttest-logloss:0.324984\n",
      "[4720]\ttrain-logloss:0.281575\ttest-logloss:0.324954\n",
      "[4730]\ttrain-logloss:0.281498\ttest-logloss:0.324929\n",
      "[4740]\ttrain-logloss:0.28144\ttest-logloss:0.32491\n",
      "[4750]\ttrain-logloss:0.281365\ttest-logloss:0.324878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4760]\ttrain-logloss:0.281277\ttest-logloss:0.324851\n",
      "[4770]\ttrain-logloss:0.281205\ttest-logloss:0.324828\n",
      "[4780]\ttrain-logloss:0.281149\ttest-logloss:0.324813\n",
      "[4790]\ttrain-logloss:0.281082\ttest-logloss:0.324798\n",
      "[4800]\ttrain-logloss:0.281004\ttest-logloss:0.324775\n",
      "[4810]\ttrain-logloss:0.280942\ttest-logloss:0.324751\n",
      "[4820]\ttrain-logloss:0.280865\ttest-logloss:0.324723\n",
      "[4830]\ttrain-logloss:0.280729\ttest-logloss:0.324647\n",
      "[4840]\ttrain-logloss:0.28064\ttest-logloss:0.324613\n",
      "[4850]\ttrain-logloss:0.280491\ttest-logloss:0.32453\n",
      "[4860]\ttrain-logloss:0.280415\ttest-logloss:0.324492\n",
      "[4870]\ttrain-logloss:0.280354\ttest-logloss:0.324468\n",
      "[4880]\ttrain-logloss:0.280282\ttest-logloss:0.324443\n",
      "[4890]\ttrain-logloss:0.28021\ttest-logloss:0.324417\n",
      "[4900]\ttrain-logloss:0.280152\ttest-logloss:0.3244\n",
      "[4910]\ttrain-logloss:0.280085\ttest-logloss:0.324374\n",
      "[4920]\ttrain-logloss:0.280011\ttest-logloss:0.324349\n",
      "[4930]\ttrain-logloss:0.279947\ttest-logloss:0.324322\n",
      "[4940]\ttrain-logloss:0.279869\ttest-logloss:0.324296\n",
      "[4950]\ttrain-logloss:0.279803\ttest-logloss:0.324268\n",
      "[4960]\ttrain-logloss:0.279728\ttest-logloss:0.32424\n",
      "[4970]\ttrain-logloss:0.279659\ttest-logloss:0.324222\n",
      "[4980]\ttrain-logloss:0.279596\ttest-logloss:0.324199\n",
      "[4990]\ttrain-logloss:0.279511\ttest-logloss:0.324161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgboost_model_5000iterations_8depth.pkl']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################\n",
    "# Running XGBoost\n",
    "##########################################\n",
    "# Set parameters for XGBoost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 8\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_test, 'test')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 5000, watchlist, early_stopping_rounds=50, verbose_eval=10)\n",
    "joblib.dump(bst, 'xgboost_model_5000iterations_8depth.pkl')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_features.pkl']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################\n",
    "# Create the test features using FeatureUnion\n",
    "##########################################\n",
    "test_features = comb_features.transform([df_test['question1'], df_test['question2']])\n",
    "\n",
    "#Merge FeatureUnion features with Magic Features\n",
    "total_test_features = scipy.sparse.hstack(blocks=[test_features, magic_test])\n",
    "\n",
    "joblib.dump(total_test_features, 'test_features.pkl')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2345796x86722 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 61634409 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Predicting using XGBoost\n",
    "##########################################\n",
    "test = xgb.DMatrix(total_test_features)\n",
    "test_prediction = bst.predict(test)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Creating Submission File\n",
    "##########################################\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = test_prediction\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check Submission File Length\n",
    "len(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(sub, 'submission.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

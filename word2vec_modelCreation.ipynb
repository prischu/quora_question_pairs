{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# word2vec_modelCreation.ipynb\n",
    "# Purpose: Word2Vec Model for Kaggle's Quora Question Pairs Competition (March 2017 - May 2017)\n",
    "# Author: Priscilla Li\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Load Required Python Libraries\n",
    "##########################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import chardet\n",
    "import itertools\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Dataset\n",
    "##########################################\n",
    "#Training Dataset\n",
    "data = pd.read_csv('train.csv')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Tokenizing Functions used to create Word2Vec Model\n",
    "##########################################\n",
    "#Tokenizer for sentence splitting\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#Function to prep question1 and question2 for word2vec model\n",
    "#Word2vec expects a list of lists as input (single sentences each as a list of words)\n",
    "def question_to_wordlist(text, remove_stopwords = False):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)\n",
    "\n",
    "def question_to_sentences(text, tokenizer, remove_stopwords = False):\n",
    "    text = tokenizer.tokenize(text.strip())\n",
    "    sentences = []\n",
    "    \n",
    "    for t in text:\n",
    "        if(len(t) > 0):\n",
    "            sentences.append(question_to_wordlist(t, remove_stopwords))\n",
    "    return sentences\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/404290 [00:00<?, ?it/s]  0%|          | 179/404290 [00:00<03:48, 1770.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404290/404290 [01:26<00:00, 4659.25it/s]%|          | 1439/404290 [00:00<02:25, 2778.23it/s]\n",
      "100%|██████████| 404290/404290 [01:30<00:00, 4455.55it/s]\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# Data Preprocessing prior to Word2Vec Model Creation\n",
    "##########################################\n",
    "#Prep data for word2vec\n",
    "sentences = []\n",
    "\n",
    "print(\"Parsing sentences from training set...\")\n",
    "#Converting question1 to sentences for word2vec model\n",
    "for i in tqdm(range(0, len(data['question1']))):\n",
    "    try:\n",
    "        #Check for empty strings \"\"\n",
    "        if(not pd.isnull(data['question1'][i])):\n",
    "            sentences += question_to_sentences(data['question1'][i], tokenizer)\n",
    "    except:\n",
    "        try:\n",
    "            encoding = chardet.detect(data['question1'][i])['encoding']\n",
    "            sentences += question_to_sentences(data['question1'][i].decode(encoding), tokenizer)\n",
    "        except:\n",
    "            print(encoding)\n",
    "\n",
    "#Converting question2 to sentences for word2vec model\n",
    "for i in tqdm(range(0,len(data['question2']))):\n",
    "    try:\n",
    "        if(not pd.isnull(data['question2'][i])):\n",
    "            sentences += question_to_sentences(data['question2'][i], tokenizer)\n",
    "    except:\n",
    "        try:\n",
    "            encoding = chardet.detect(data['question2'][i])['encoding']\n",
    "            sentences += question_to_sentences(data['question2'][i].decode(encoding), tokenizer)\n",
    "        except:\n",
    "            print(encoding)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 00:02:24,966 : INFO : collecting all words and their counts\n",
      "2017-05-16 00:02:24,968 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-16 00:02:25,021 : INFO : PROGRESS: at sentence #10000, processed 99111 words, keeping 10771 word types\n",
      "2017-05-16 00:02:25,071 : INFO : PROGRESS: at sentence #20000, processed 198901 words, keeping 15372 word types\n",
      "2017-05-16 00:02:25,105 : INFO : PROGRESS: at sentence #30000, processed 297847 words, keeping 18786 word types\n",
      "2017-05-16 00:02:25,147 : INFO : PROGRESS: at sentence #40000, processed 396378 words, keeping 21570 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training word2vec model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 00:02:25,192 : INFO : PROGRESS: at sentence #50000, processed 495897 words, keeping 23996 word types\n",
      "2017-05-16 00:02:25,236 : INFO : PROGRESS: at sentence #60000, processed 595251 words, keeping 26209 word types\n",
      "2017-05-16 00:02:25,279 : INFO : PROGRESS: at sentence #70000, processed 693633 words, keeping 28128 word types\n",
      "2017-05-16 00:02:25,322 : INFO : PROGRESS: at sentence #80000, processed 793440 words, keeping 29888 word types\n",
      "2017-05-16 00:02:25,363 : INFO : PROGRESS: at sentence #90000, processed 892903 words, keeping 31430 word types\n",
      "2017-05-16 00:02:25,403 : INFO : PROGRESS: at sentence #100000, processed 992033 words, keeping 32934 word types\n",
      "2017-05-16 00:02:25,451 : INFO : PROGRESS: at sentence #110000, processed 1091774 words, keeping 34379 word types\n",
      "2017-05-16 00:02:25,492 : INFO : PROGRESS: at sentence #120000, processed 1190774 words, keeping 35739 word types\n",
      "2017-05-16 00:02:25,535 : INFO : PROGRESS: at sentence #130000, processed 1289539 words, keeping 36976 word types\n",
      "2017-05-16 00:02:25,581 : INFO : PROGRESS: at sentence #140000, processed 1389467 words, keeping 38253 word types\n",
      "2017-05-16 00:02:25,628 : INFO : PROGRESS: at sentence #150000, processed 1488913 words, keeping 39433 word types\n",
      "2017-05-16 00:02:25,674 : INFO : PROGRESS: at sentence #160000, processed 1588691 words, keeping 40578 word types\n",
      "2017-05-16 00:02:25,723 : INFO : PROGRESS: at sentence #170000, processed 1688473 words, keeping 41718 word types\n",
      "2017-05-16 00:02:25,770 : INFO : PROGRESS: at sentence #180000, processed 1787963 words, keeping 42857 word types\n",
      "2017-05-16 00:02:25,819 : INFO : PROGRESS: at sentence #190000, processed 1887555 words, keeping 43853 word types\n",
      "2017-05-16 00:02:25,859 : INFO : PROGRESS: at sentence #200000, processed 1986905 words, keeping 44860 word types\n",
      "2017-05-16 00:02:25,903 : INFO : PROGRESS: at sentence #210000, processed 2086787 words, keeping 45826 word types\n",
      "2017-05-16 00:02:25,938 : INFO : PROGRESS: at sentence #220000, processed 2185048 words, keeping 46791 word types\n",
      "2017-05-16 00:02:25,985 : INFO : PROGRESS: at sentence #230000, processed 2283972 words, keeping 47692 word types\n",
      "2017-05-16 00:02:26,038 : INFO : PROGRESS: at sentence #240000, processed 2382689 words, keeping 48528 word types\n",
      "2017-05-16 00:02:26,078 : INFO : PROGRESS: at sentence #250000, processed 2482567 words, keeping 49427 word types\n",
      "2017-05-16 00:02:26,126 : INFO : PROGRESS: at sentence #260000, processed 2582084 words, keeping 50217 word types\n",
      "2017-05-16 00:02:26,176 : INFO : PROGRESS: at sentence #270000, processed 2682208 words, keeping 51022 word types\n",
      "2017-05-16 00:02:26,224 : INFO : PROGRESS: at sentence #280000, processed 2781482 words, keeping 51871 word types\n",
      "2017-05-16 00:02:26,272 : INFO : PROGRESS: at sentence #290000, processed 2880857 words, keeping 52687 word types\n",
      "2017-05-16 00:02:26,315 : INFO : PROGRESS: at sentence #300000, processed 2980885 words, keeping 53439 word types\n",
      "2017-05-16 00:02:26,363 : INFO : PROGRESS: at sentence #310000, processed 3080129 words, keeping 54174 word types\n",
      "2017-05-16 00:02:26,402 : INFO : PROGRESS: at sentence #320000, processed 3179419 words, keeping 54939 word types\n",
      "2017-05-16 00:02:26,452 : INFO : PROGRESS: at sentence #330000, processed 3278142 words, keeping 55691 word types\n",
      "2017-05-16 00:02:26,501 : INFO : PROGRESS: at sentence #340000, processed 3377418 words, keeping 56449 word types\n",
      "2017-05-16 00:02:26,552 : INFO : PROGRESS: at sentence #350000, processed 3477241 words, keeping 57137 word types\n",
      "2017-05-16 00:02:26,587 : INFO : PROGRESS: at sentence #360000, processed 3576792 words, keeping 57883 word types\n",
      "2017-05-16 00:02:26,634 : INFO : PROGRESS: at sentence #370000, processed 3675881 words, keeping 58548 word types\n",
      "2017-05-16 00:02:26,681 : INFO : PROGRESS: at sentence #380000, processed 3775009 words, keeping 59156 word types\n",
      "2017-05-16 00:02:26,714 : INFO : PROGRESS: at sentence #390000, processed 3874933 words, keeping 59726 word types\n",
      "2017-05-16 00:02:26,763 : INFO : PROGRESS: at sentence #400000, processed 3973785 words, keeping 60348 word types\n",
      "2017-05-16 00:02:26,808 : INFO : PROGRESS: at sentence #410000, processed 4073329 words, keeping 60963 word types\n",
      "2017-05-16 00:02:26,854 : INFO : PROGRESS: at sentence #420000, processed 4173251 words, keeping 61581 word types\n",
      "2017-05-16 00:02:26,899 : INFO : PROGRESS: at sentence #430000, processed 4273533 words, keeping 62251 word types\n",
      "2017-05-16 00:02:26,946 : INFO : PROGRESS: at sentence #440000, processed 4373736 words, keeping 62833 word types\n",
      "2017-05-16 00:02:26,992 : INFO : PROGRESS: at sentence #450000, processed 4473310 words, keeping 63376 word types\n",
      "2017-05-16 00:02:27,027 : INFO : PROGRESS: at sentence #460000, processed 4571778 words, keeping 63806 word types\n",
      "2017-05-16 00:02:27,075 : INFO : PROGRESS: at sentence #470000, processed 4670379 words, keeping 64234 word types\n",
      "2017-05-16 00:02:27,108 : INFO : PROGRESS: at sentence #480000, processed 4769785 words, keeping 64655 word types\n",
      "2017-05-16 00:02:27,153 : INFO : PROGRESS: at sentence #490000, processed 4868727 words, keeping 65032 word types\n",
      "2017-05-16 00:02:27,200 : INFO : PROGRESS: at sentence #500000, processed 4968327 words, keeping 65422 word types\n",
      "2017-05-16 00:02:27,243 : INFO : PROGRESS: at sentence #510000, processed 5067717 words, keeping 65815 word types\n",
      "2017-05-16 00:02:27,288 : INFO : PROGRESS: at sentence #520000, processed 5166187 words, keeping 66219 word types\n",
      "2017-05-16 00:02:27,322 : INFO : PROGRESS: at sentence #530000, processed 5265570 words, keeping 66600 word types\n",
      "2017-05-16 00:02:27,371 : INFO : PROGRESS: at sentence #540000, processed 5364539 words, keeping 66993 word types\n",
      "2017-05-16 00:02:27,418 : INFO : PROGRESS: at sentence #550000, processed 5463718 words, keeping 67374 word types\n",
      "2017-05-16 00:02:27,464 : INFO : PROGRESS: at sentence #560000, processed 5563461 words, keeping 67784 word types\n",
      "2017-05-16 00:02:27,508 : INFO : PROGRESS: at sentence #570000, processed 5662457 words, keeping 68136 word types\n",
      "2017-05-16 00:02:27,542 : INFO : PROGRESS: at sentence #580000, processed 5761330 words, keeping 68492 word types\n",
      "2017-05-16 00:02:27,590 : INFO : PROGRESS: at sentence #590000, processed 5860702 words, keeping 68849 word types\n",
      "2017-05-16 00:02:27,638 : INFO : PROGRESS: at sentence #600000, processed 5960325 words, keeping 69227 word types\n",
      "2017-05-16 00:02:27,672 : INFO : PROGRESS: at sentence #610000, processed 6060125 words, keeping 69621 word types\n",
      "2017-05-16 00:02:27,724 : INFO : PROGRESS: at sentence #620000, processed 6159316 words, keeping 70001 word types\n",
      "2017-05-16 00:02:27,759 : INFO : PROGRESS: at sentence #630000, processed 6258609 words, keeping 70368 word types\n",
      "2017-05-16 00:02:27,817 : INFO : PROGRESS: at sentence #640000, processed 6357886 words, keeping 70714 word types\n",
      "2017-05-16 00:02:27,852 : INFO : PROGRESS: at sentence #650000, processed 6458141 words, keeping 71063 word types\n",
      "2017-05-16 00:02:27,898 : INFO : PROGRESS: at sentence #660000, processed 6557572 words, keeping 71400 word types\n",
      "2017-05-16 00:02:27,947 : INFO : PROGRESS: at sentence #670000, processed 6655519 words, keeping 71713 word types\n",
      "2017-05-16 00:02:28,030 : INFO : PROGRESS: at sentence #680000, processed 6755066 words, keeping 72057 word types\n",
      "2017-05-16 00:02:28,194 : INFO : PROGRESS: at sentence #690000, processed 6854194 words, keeping 72382 word types\n",
      "2017-05-16 00:02:28,311 : INFO : PROGRESS: at sentence #700000, processed 6953486 words, keeping 72772 word types\n",
      "2017-05-16 00:02:28,387 : INFO : PROGRESS: at sentence #710000, processed 7052460 words, keeping 73100 word types\n",
      "2017-05-16 00:02:28,438 : INFO : PROGRESS: at sentence #720000, processed 7151959 words, keeping 73453 word types\n",
      "2017-05-16 00:02:28,473 : INFO : PROGRESS: at sentence #730000, processed 7250740 words, keeping 73832 word types\n",
      "2017-05-16 00:02:28,530 : INFO : PROGRESS: at sentence #740000, processed 7350964 words, keeping 74152 word types\n",
      "2017-05-16 00:02:28,587 : INFO : PROGRESS: at sentence #750000, processed 7451346 words, keeping 74475 word types\n",
      "2017-05-16 00:02:28,642 : INFO : PROGRESS: at sentence #760000, processed 7550516 words, keeping 74790 word types\n",
      "2017-05-16 00:02:28,697 : INFO : PROGRESS: at sentence #770000, processed 7649844 words, keeping 75107 word types\n",
      "2017-05-16 00:02:28,759 : INFO : PROGRESS: at sentence #780000, processed 7749287 words, keeping 75446 word types\n",
      "2017-05-16 00:02:28,824 : INFO : PROGRESS: at sentence #790000, processed 7848202 words, keeping 75770 word types\n",
      "2017-05-16 00:02:28,860 : INFO : PROGRESS: at sentence #800000, processed 7948124 words, keeping 76062 word types\n",
      "2017-05-16 00:02:28,949 : INFO : PROGRESS: at sentence #810000, processed 8046570 words, keeping 76366 word types\n",
      "2017-05-16 00:02:29,213 : INFO : PROGRESS: at sentence #820000, processed 8146532 words, keeping 76646 word types\n",
      "2017-05-16 00:02:29,406 : INFO : PROGRESS: at sentence #830000, processed 8245809 words, keeping 76949 word types\n",
      "2017-05-16 00:02:29,452 : INFO : PROGRESS: at sentence #840000, processed 8344670 words, keeping 77290 word types\n",
      "2017-05-16 00:02:29,521 : INFO : PROGRESS: at sentence #850000, processed 8444613 words, keeping 77578 word types\n",
      "2017-05-16 00:02:29,592 : INFO : PROGRESS: at sentence #860000, processed 8544007 words, keeping 77887 word types\n",
      "2017-05-16 00:02:29,656 : INFO : PROGRESS: at sentence #870000, processed 8643822 words, keeping 78180 word types\n",
      "2017-05-16 00:02:29,719 : INFO : PROGRESS: at sentence #880000, processed 8743213 words, keeping 78497 word types\n",
      "2017-05-16 00:02:29,765 : INFO : PROGRESS: at sentence #890000, processed 8843568 words, keeping 78790 word types\n",
      "2017-05-16 00:02:29,833 : INFO : PROGRESS: at sentence #900000, processed 8943601 words, keeping 79108 word types\n",
      "2017-05-16 00:02:29,870 : INFO : collected 79225 word types from a corpus of 8976428 raw words and 903278 sentences\n",
      "2017-05-16 00:02:29,882 : INFO : Loading a fresh vocabulary\n",
      "2017-05-16 00:02:30,199 : INFO : min_count=10 retains 20224 unique words (25% of original 79225, drops 59001)\n",
      "2017-05-16 00:02:30,200 : INFO : min_count=10 leaves 8834858 word corpus (98% of original 8976428, drops 141570)\n",
      "2017-05-16 00:02:30,339 : INFO : deleting the raw counts dictionary of 79225 items\n",
      "2017-05-16 00:02:30,350 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2017-05-16 00:02:30,352 : INFO : downsampling leaves estimated 6214323 word corpus (70.3% of prior 8834858)\n",
      "2017-05-16 00:02:30,354 : INFO : estimated required memory for 20224 words and 300 dimensions: 58649600 bytes\n",
      "2017-05-16 00:02:30,500 : INFO : resetting layer weights\n",
      "2017-05-16 00:02:31,204 : INFO : training model with 4 workers on 20224 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-16 00:02:31,205 : INFO : expecting 903278 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-16 00:02:32,246 : INFO : PROGRESS: at 1.00% examples, 306645 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:33,254 : INFO : PROGRESS: at 2.23% examples, 341864 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:34,257 : INFO : PROGRESS: at 3.43% examples, 351985 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 00:02:35,268 : INFO : PROGRESS: at 4.63% examples, 356383 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:36,273 : INFO : PROGRESS: at 5.79% examples, 356521 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:37,276 : INFO : PROGRESS: at 6.64% examples, 340781 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:38,279 : INFO : PROGRESS: at 7.24% examples, 318682 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:39,390 : INFO : PROGRESS: at 7.93% examples, 301524 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:40,395 : INFO : PROGRESS: at 8.75% examples, 296328 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:41,396 : INFO : PROGRESS: at 9.82% examples, 299792 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 00:02:42,507 : INFO : PROGRESS: at 10.85% examples, 298507 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:43,524 : INFO : PROGRESS: at 11.63% examples, 293561 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:44,556 : INFO : PROGRESS: at 12.68% examples, 295251 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:45,571 : INFO : PROGRESS: at 13.83% examples, 299483 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:46,597 : INFO : PROGRESS: at 14.99% examples, 302957 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:47,599 : INFO : PROGRESS: at 16.11% examples, 305557 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:48,616 : INFO : PROGRESS: at 17.26% examples, 308385 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:49,624 : INFO : PROGRESS: at 17.84% examples, 301271 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:50,628 : INFO : PROGRESS: at 18.44% examples, 295317 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-16 00:02:51,636 : INFO : PROGRESS: at 19.29% examples, 293614 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:52,665 : INFO : PROGRESS: at 20.35% examples, 295018 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:53,677 : INFO : PROGRESS: at 21.45% examples, 296804 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:54,682 : INFO : PROGRESS: at 22.40% examples, 296762 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:55,724 : INFO : PROGRESS: at 23.21% examples, 294300 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:56,745 : INFO : PROGRESS: at 24.21% examples, 294709 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:57,752 : INFO : PROGRESS: at 25.32% examples, 296546 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 00:02:58,759 : INFO : PROGRESS: at 26.23% examples, 295991 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:02:59,764 : INFO : PROGRESS: at 27.24% examples, 296460 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:00,779 : INFO : PROGRESS: at 28.06% examples, 294931 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:01,780 : INFO : PROGRESS: at 28.86% examples, 293404 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:02,786 : INFO : PROGRESS: at 29.70% examples, 292376 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:03,799 : INFO : PROGRESS: at 30.73% examples, 293063 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:04,825 : INFO : PROGRESS: at 31.53% examples, 291541 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:05,850 : INFO : PROGRESS: at 32.34% examples, 290108 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:06,855 : INFO : PROGRESS: at 33.34% examples, 290679 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:07,866 : INFO : PROGRESS: at 34.45% examples, 292118 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:08,868 : INFO : PROGRESS: at 35.46% examples, 292635 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:09,874 : INFO : PROGRESS: at 36.53% examples, 293625 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:10,893 : INFO : PROGRESS: at 37.64% examples, 294824 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:11,904 : INFO : PROGRESS: at 38.66% examples, 295327 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:12,960 : INFO : PROGRESS: at 39.66% examples, 295320 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:13,971 : INFO : PROGRESS: at 40.53% examples, 294648 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 00:03:15,006 : INFO : PROGRESS: at 41.58% examples, 295099 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:16,062 : INFO : PROGRESS: at 42.53% examples, 294772 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:17,083 : INFO : PROGRESS: at 43.60% examples, 295445 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:18,096 : INFO : PROGRESS: at 44.47% examples, 294814 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:19,101 : INFO : PROGRESS: at 45.23% examples, 293531 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 00:03:20,115 : INFO : PROGRESS: at 46.01% examples, 292384 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:21,139 : INFO : PROGRESS: at 46.72% examples, 290815 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:22,147 : INFO : PROGRESS: at 47.63% examples, 290609 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:23,153 : INFO : PROGRESS: at 48.59% examples, 290702 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:24,164 : INFO : PROGRESS: at 49.55% examples, 290761 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:25,167 : INFO : PROGRESS: at 50.59% examples, 291390 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:26,173 : INFO : PROGRESS: at 51.64% examples, 291986 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:27,181 : INFO : PROGRESS: at 52.67% examples, 292426 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:28,186 : INFO : PROGRESS: at 53.72% examples, 292988 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:29,188 : INFO : PROGRESS: at 54.76% examples, 293545 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:30,202 : INFO : PROGRESS: at 55.81% examples, 294018 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:31,230 : INFO : PROGRESS: at 56.85% examples, 294413 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 00:03:32,256 : INFO : PROGRESS: at 57.93% examples, 294915 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:33,259 : INFO : PROGRESS: at 58.93% examples, 295179 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:34,271 : INFO : PROGRESS: at 59.97% examples, 295613 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:35,276 : INFO : PROGRESS: at 60.86% examples, 295287 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 00:03:36,278 : INFO : PROGRESS: at 61.80% examples, 295198 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:37,295 : INFO : PROGRESS: at 62.82% examples, 295461 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:38,322 : INFO : PROGRESS: at 63.85% examples, 295675 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:39,366 : INFO : PROGRESS: at 64.89% examples, 295912 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:40,367 : INFO : PROGRESS: at 65.92% examples, 296220 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:41,385 : INFO : PROGRESS: at 66.98% examples, 296648 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:42,390 : INFO : PROGRESS: at 68.03% examples, 297018 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:43,408 : INFO : PROGRESS: at 68.88% examples, 296467 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:44,409 : INFO : PROGRESS: at 69.83% examples, 296473 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:45,416 : INFO : PROGRESS: at 70.77% examples, 296372 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:46,439 : INFO : PROGRESS: at 71.71% examples, 296212 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:47,456 : INFO : PROGRESS: at 72.69% examples, 296250 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:48,471 : INFO : PROGRESS: at 73.73% examples, 296582 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:49,526 : INFO : PROGRESS: at 74.76% examples, 296654 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:50,534 : INFO : PROGRESS: at 75.81% examples, 296992 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:51,543 : INFO : PROGRESS: at 76.85% examples, 297318 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:52,560 : INFO : PROGRESS: at 77.70% examples, 296838 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:53,569 : INFO : PROGRESS: at 78.63% examples, 296730 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:54,585 : INFO : PROGRESS: at 79.66% examples, 296933 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:55,597 : INFO : PROGRESS: at 80.52% examples, 296568 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:56,601 : INFO : PROGRESS: at 81.35% examples, 296073 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 00:03:57,625 : INFO : PROGRESS: at 82.28% examples, 295924 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:58,654 : INFO : PROGRESS: at 83.29% examples, 295996 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:03:59,669 : INFO : PROGRESS: at 84.33% examples, 296273 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:04:00,675 : INFO : PROGRESS: at 85.34% examples, 296417 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:04:01,689 : INFO : PROGRESS: at 86.33% examples, 296531 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:04:02,702 : INFO : PROGRESS: at 87.36% examples, 296720 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:04:03,743 : INFO : PROGRESS: at 88.38% examples, 296812 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:04:04,747 : INFO : PROGRESS: at 89.38% examples, 296956 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:04:05,750 : INFO : PROGRESS: at 90.41% examples, 297169 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:04:06,775 : INFO : PROGRESS: at 91.41% examples, 297246 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:04:07,785 : INFO : PROGRESS: at 92.46% examples, 297510 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:04:08,789 : INFO : PROGRESS: at 93.40% examples, 297434 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 00:04:09,811 : INFO : PROGRESS: at 94.35% examples, 297375 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:04:10,825 : INFO : PROGRESS: at 95.33% examples, 297412 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:04:11,829 : INFO : PROGRESS: at 96.29% examples, 297405 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:04:12,849 : INFO : PROGRESS: at 97.32% examples, 297557 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 00:04:13,853 : INFO : PROGRESS: at 98.36% examples, 297816 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 00:04:14,854 : INFO : PROGRESS: at 99.12% examples, 297212 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 00:04:15,725 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 00:04:15,750 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 00:04:15,758 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 00:04:15,762 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 00:04:15,764 : INFO : training on 44882140 raw words (31074571 effective words) took 104.5s, 297261 effective words/s\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# Training Word2Vec Model\n",
    "##########################################\n",
    "#Set up log messaging while word2vec model is being trained\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#Set parameters for word2vec model\n",
    "num_features = 300 #Word vector dimensionality\n",
    "min_word_count = 10 #Minimum word count \n",
    "num_workers = 4 #Number of threads to run in parallel\n",
    "context = 5 #Context window size\n",
    "downsampling = 1e-3 #Downsample setting for frequent words\n",
    "\n",
    "#Training word2vec model\n",
    "print(\"Training word2vec model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers = num_workers,\\\n",
    "                         size = num_features, min_count = min_word_count,\\\n",
    "                         window = context, sample = downsampling)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 00:05:22,470 : INFO : saving Word2Vec object under 300features_10minwords_5context_inprogress, separately None\n",
      "2017-05-16 00:05:22,474 : INFO : not storing attribute syn0norm\n",
      "2017-05-16 00:05:22,476 : INFO : not storing attribute cum_table\n",
      "2017-05-16 00:05:24,354 : INFO : saved 300features_10minwords_5context_inprogress\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# Saving Trained Word2Vec Model\n",
    "##########################################\n",
    "#Save the model (Can still be trained on further)\n",
    "model_name = \"300features_10minwords_5context_inprogress\"\n",
    "model.save(model_name)\n",
    "\n",
    "#If not training the model any further call init_sims to make the model memory-efficient\n",
    "# model.init_sims(replace = True)\n",
    "# model_name = \"300features_10minwords_5context\"\n",
    "# model.save(model_name)\n",
    "##########################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
